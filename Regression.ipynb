{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Theory Questions"
      ],
      "metadata": {
        "id": "EzdXxUdCJSQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "  - Simple Linear Regression is a statistical technique used to model the relationship between one independent variable and one dependent variable. It assumes this relationship can be represented by a straight line. The method helps in predicting the value of the dependent variable based on the independent variable.\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "  - Simple Linear Regression assumes that the relationship between variables is linear, the errors are independent, and the variance of errors is constant. It also assumes that the residuals are normally distributed. These assumptions help ensure reliable and valid predictions.\n",
        "\n",
        "3. What does the coefficient m represent in Y = mX + c?\n",
        "  - The coefficient m represents the slope of the regression line. It indicates how much the dependent variable Y changes when the independent variable X increases by one unit. It shows the direction and strength of the relationship.\n",
        "\n",
        "4. What does the intercept c represent in Y = mX + c?\n",
        "  - The intercept c represents the value of Y when X is zero. It shows where the regression line crosses the Y-axis. This provides a starting point for the relationship between the variables.\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "  - The slope m is calculated using the covariance between X and Y divided by the variance of X. This calculation finds how changes in X are associated with changes in Y. It gives the best-fitting line through the data points.\n",
        "\n",
        "6. What is the purpose of the least squares method?\n",
        "  - The least squares method is used to find the best-fitting regression line by minimizing the sum of squared differences between observed and predicted values. It ensures that the regression line is as close as possible to the data points.  \n",
        "\n",
        "7. How is R² interpreted?\n",
        "  - R², also called the coefficient of determination, measures how much of the variation in the dependent variable is explained by the independent variable. A higher R² value means the model explains more of the data variation.\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        "  - Multiple Linear Regression is a technique used to model the relationship between one dependent variable and two or more independent variables. It helps analyze how several factors together influence the outcome.\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "  - Simple Linear Regression uses only one independent variable to predict the dependent variable, while Multiple Linear Regression uses two or more independent variables. This makes Multiple Regression more suitable for complex real-world problems.\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "  - Multiple Linear Regression assumes linear relationships between variables, no multicollinearity among predictors, constant variance of errors, independence of observations, and normally distributed residuals. These assumptions ensure accurate and unbiased estimates.\n",
        "\n",
        "11. What is heteroscedasticity?\n",
        "  - Heteroscedasticity occurs when the variability of errors is not constant across all levels of the independent variables. It can lead to inefficient and biased estimates, reducing the reliability of the regression results.\n",
        "\n",
        "12. How can multicollinearity be reduced?\n",
        "  - Multicollinearity can be reduced by removing or combining highly correlated independent variables. Using techniques such as ridge regression or principal component analysis can also help improve the model.\n",
        "\n",
        "13. How are categorical variables used in regression?\n",
        "  - Categorical variables are transformed into numerical form using techniques like dummy coding or one-hot encoding. This allows regression models to include non-numeric data.  \n",
        "\n",
        "14. What is the role of interaction terms?\n",
        "  - Interaction terms allow the effect of one independent variable to depend on another variable. They help capture more complex relationships between predictors and the dependent variable.\n",
        "\n",
        "15. How does the interpretation of the intercept differ?\n",
        "  - In Simple Linear Regression, the intercept is the value of Y when X is zero. In Multiple Linear Regression, it represents the value of Y when all independent variables are zero, which may or may not be meaningful.\n",
        "\n",
        "16. What is the significance of the slope?\n",
        "  - The slope indicates the rate at which the dependent variable changes with the independent variable. It plays a key role in making predictions and understanding the relationship between variables.    \n",
        "\n",
        "17. How does the intercept provide context?\n",
        "  - The intercept provides a baseline level of the dependent variable when all predictors are zero. It helps in understanding where the regression line starts.\n",
        "\n",
        "18. What are the limitations of R²?\n",
        "  - R² does not indicate whether the model is correct or whether important variables are missing. It always increases when more variables are added, even if they do not improve the model.\n",
        "\n",
        "19. How do you interpret a large standard error?\n",
        "  - A large standard error means the estimated coefficient is not very precise. This indicates uncertainty and reduces confidence in the reliability of the coefficient.\n",
        "\n",
        "20. How is heteroscedasticity identified?\n",
        "  - Heteroscedasticity can be seen in residual plots as a funnel or uneven spread of errors. It is important to address it because it affects the accuracy of hypothesis tests and confidence intervals.  \n",
        "\n",
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "  - If a model has a high R² but a low adjusted R², it means that although the model appears to fit the data well, many of the variables may not actually be useful. Adjusted R² penalizes unnecessary predictors, so a low value indicates overfitting. This suggests that some independent variables do not significantly contribute to explaining the dependent variable.\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "  - Scaling is important because variables may have different units and ranges, which can affect the stability and interpretation of the model. It helps algorithms converge faster and improves numerical accuracy. Scaling is especially useful when regularization methods like ridge or lasso regression are used.\n",
        "\n",
        "23. What is polynomial regression?\n",
        "  - Polynomial regression is a type of regression where the relationship between the independent and dependent variables is modeled as a polynomial equation. It is used when data shows a curved or non-linear trend rather than a straight line.  \n",
        "\n",
        "24. How does polynomial regression differ from linear regression?\n",
        "  - Linear regression fits a straight line to the data, while polynomial regression fits a curved line. Polynomial regression includes higher-order terms such as X², X³, and so on to better model complex patterns.\n",
        "\n",
        "25. When is polynomial regression used?\n",
        "  - Polynomial regression is used when the relationship between variables is non-linear but still smooth and continuous. It is especially helpful when data trends form curves instead of straight lines.  \n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        "\n",
        "  - The general equation is Y=b\n",
        "0\n",
        "+b\n",
        "1\n",
        "X+b\n",
        "2\n",
        "X\n",
        "2\n",
        "+b\n",
        "3\n",
        "X\n",
        "3\n",
        "+⋯+b\n",
        "n\n",
        "X\n",
        "n\n",
        ". Here, n represents the degree of the polynomial, and the coefficients determine the shape of the curve.\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        "  - Yes, polynomial regression can be extended to multiple variables by including polynomial and interaction terms for each variable. This allows the model to capture more complex relationships among predictors.\n",
        "\n",
        "28. What are the limitations of polynomial regression?\n",
        "  - Polynomial regression can easily overfit the data, especially with a high-degree polynomial. It is also sensitive to outliers and may perform poorly when predicting outside the range of the data.\n",
        "\n",
        "29. How can we evaluate model fit when selecting the degree of a polynomial?\n",
        "  - Model fit can be evaluated using methods such as R², adjusted R², cross-validation, and mean squared error. These techniques help determine whether the polynomial degree is too low or too high.\n",
        "\n",
        "30. Why is visualization important in polynomial regression?\n",
        "  - Visualization helps in understanding how well the polynomial curve fits the data. It allows us to see overfitting, underfitting, and the overall pattern of the relationship between variables.\n",
        "\n",
        "31. How is polynomial regression implemented in Python?\n",
        "  - Polynomial regression in Python is typically implemented using libraries such as NumPy and scikit-learn. The PolynomialFeatures class is used to generate polynomial terms, and then a linear regression model is applied to fit the data.  "
      ],
      "metadata": {
        "id": "cav9MKw4JWiy"
      }
    }
  ]
}